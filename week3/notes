Data Structures
. eg, lists, stacks, queues, heaps, search trees, hash tables,
  bloom filters, union-find
. different structures support different sets of
  operations
. fewer operations a structure supports, the faster the operations
  will be, and the smaller amount of memory required

Heaps
. operations:
  -- insertion -- add a new object
     -- O(log n)
  -- extract min -- remove an object in heap with a minimum key value 
     ( ties are broken arbitrarily ); there can also be extract max 
     operations, but not in same heap
     -- O(log n)
 -- heapify -- initialize a heap with a BATCH of
    objects in linear time
 -- delete (O(log n))
. container for objects that have keys;
  keys can be compared to one another
. canonical uses:
  -- your program is doing repetitive minimum computations
     using brute-force search --> SORTING!

Heap Sort
1) insert all n array elements into a heap
2) Extract-min to pluck out elements in sorted order
  -- O(n log n)
  -- optimal for a comparison-based sorting algorithm
  -- not quite as good as quicksort

Applications of Heaps
. "Priority queues" for Event Managers
 -- time stamp is the key
 -- extract-min yields the next scheduled event
. "Median maintenance"
 -- given a sequence of numbers, find median
    of subsets of the list (eg, median of first 2, median of first 3, median of first 4,
    etc)
 -- use 2 heaps
  -- smallest half of numbers in first heap
  -- largest half in second heap
  -- median is either largest of first, smallest of 2nd, or both
  -- periodically may have to move items between
     heaps to maintain balance
. Dijkstra

Implementation of Heaps
. best thought of as binary trees,
  but best implemented as __array__
 -- just go through the heap starting at level 0,
    left to right, adding items to array
 -- parent node of element i is:
     i/2 if i is even
     floor(i/2) if i is odd
 -- children of i:
    2i, 2i+1

. Insert -- done in logarithmic_2 time
 1) make the new key you're inserting the last leaf at the last level
 2) "Bubble" up until heap property is restored

. Extract-Min -- done in logarithmic_2 time
 1) "extract" root and delete it
 2) Move LAST LEAF to be new root -- this will likely result in lots
    of heap violations, so...
 3) Iteratively "Bubble Down" so that we again meet criterion that
    all keys are <= their children
    -- Bubble down as needed by replacing the root with the SMALLER
       of its children

Balanced Search Trees
 -- act like a sorted array, with fast operations for inserts and deletes
 -- needed for dynamic data structures -- if data static, just use sorted array
. SEARCH: O(log n)
. SELECT: O(log n)
. MIN/MAX: O(log n)
. PRED/SUCC (PREDECESSSOR/SUCCESSOR): O(log n)
. OUTPUT IN SORTED ORDER: O(n)
. RANK: O(log n)
. INSERT: O(log n)
. DELETE: O(log n)

Structure of a search tree
. Assume 3 pointers -- 1 to parent, 1 to left/right child each, may be null
. SEARCH TREE PROPERTY
 -- if node x has a key value, all keys in left subtree < x, all keys in right subtree > x
 -- there can be duplicate keys, just need convention for how handle 'ties'
 -- helps with fast SEARCHING, which is the whole point
. many possible trees for a given set of keys
 -- Height, then, can vary from ideal case of height = log_2n to height = n-1 (single node in each level)
. to search:
  -- start at root, recurse left or right if desired key is larger or smaller;
     return pointer, may be null
. to insert:
  -- do a search, which will fail (assuming no duplicates); set
     the null pointer to desired value
. worst-case running time of search:
 -- O(height) -- b/c unbalanced trees are possible

Operations;
. Minimum:
 -- start at root, keep visiting left pointers until last key found
. Predecessor: (of key k)
 -- if k's left subtree nonempty, return max key in left subtree
 -- if left subtree empty, follow parent pointers until you get to something smaller, which is the predecessor
 -- flip left and right in above 2 points to get successor
 -- worst case running time is O(height)
. In-order traversal: (print out keys in increasing order)
 -- let r = root, with left and right subtrees T_L and T_R
 1) recurse on T_L
 2) print out r
 3) recurse on T_R
  -- n recursive calls, each prints one node -- O(n)
. Deletion (node k):
 -- if no children, just delete node k
 -- if k has one child: replace k with its child
 -- if k has two children:
   -- compute k's predecessor, l (go left, then take first right and keep going until no more right nodes)
   -- swap k and l
   -- delete k (which will be a case with one or no children)
 -- worst-case running time: O(height) again

. Augment your data structure -- for each node, add some additional info
  -- eg, add size field, number of tree nodes in subtree rooted at a given node
  -- note -- you have to keep this data up-to-date as tree changes
    -- eg, with insertion or deletion, you have to go back up the tree to increment/decrement size
. Select: (give me the i'th smallest value)
 -- depends on size of subtrees, which is why you would augment with that property
 -- let a = number of nodes in left subtree (0 if none)
 -- if i <= a, recurse on left subtree to find i'th element
 -- if i == a+1, it's x, do that
 -- if i > a+1, recurse on right tree
 -- running time: O(height)
. Rank: (how many nodes are less than i)

Balanced Search Trees
. maintain balanced treesm thus guaranteeing logarithmic, rather than linear, height
. different approaches to maintaining the height
 -- e.g., red/black
 -- all have different 'invariants' involved in maintaining the height

Red/Black Invariants
1) each node stores one extra bit of info -- red/black node indicator
2) Root is black
3) no 2 reds in a row -- e.g., a red node's children / parents must be black
4) every path from root to a null pointer (ie, an unsuccessful search) must
   traverse the same number of black nodes
. every red/black tree with n nodes has height <= 2log_2(n+1)

