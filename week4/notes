Hash tables
. like arrays
 -- immediate random access
 -- ... except, your indexes aren't integers
. maintain a possibly evolving set of stuff
. aka a dictionary
 -- but does NOT maintain a predictable ordering
. Operations
 -- insert
 -- delete
 -- lookup
 *** guarantee: all done in constant O(1) time
. caveats:
 1) properly implemented
 2) data not pathological
. interesting history tidbit: hash tables first devised building compilers in 1950s; needed 
quick way to look up symbols and determine whether previously defined or not
. application: remembering (and looking up) previous steps
 -- i.e., memoization

Implementation
. Setup: identify the 'universe' u of possible things that could be in your dataset
 -- could be huge -- eg, 2^32 IP addresses in router
. Goal is to maintain evolving set s <= u

Naive Solutions
. A simple array would allow constant time access, but space
  requirements proportional to universe
. Use a list -- size can be proportional to s, 
  but list traversal is still proportional to length of list

Better Solution:
. an array about 2x size of s
 -- each entry in array is a 'bucket'
. assume set size s doesn't fluctuate too much, but you
  can do things like double size of array when s starts
  to get too big -- not important for understanding implementation
  though

Hash function
. takes a desired key and maps it to position in array
 -- input a key, output position in array
. hash function tells us in what position we should store
  the item

Collisions
. what if hash function gives us same output
  for different inputs?
 -- ie, only takes 23 people in a room before you're 50% likely
    to have 2 people with same birth date
. effectively a compression function, in that it takes the universe of
  u and maps it to much smaller set
. solution #1: "Chaining"
   -- each 'bucket' in array is a list on which you perform
   -- insert, update, delete as needed
. solution #2: 'open addressing'
  -- only one object per bucket
  -- hash 'sequence'
  -- if first choice is collision, two options:
     1) call function again, this gives you 'additive',
        you keep adding this number to bucket choice
        -- e.g., if 17 is first, then 23, you try 40, then 63, then 86, etc.
     2) increment by one each time you hit a collision until you find a free
        bucket
  -- con: deletion is trickier
. solution #3: implement both, see which performs better

Good Hash Functions
. data should be spread out amongst buckets
 -- ideally, completely random
. hash function itself should have good performance
 -- constant time
. should be easy to store -- e.g., constant space used regardless of n

Bad Hash Functions
. example: 
 phone numbers -- u = 10^10
 say you expect 500 numbers to store
 set n = 1000
 you need a function that will take any input
    and spit out a number between 0 and 999
. terrible hash function would use area code as return value of hash function
 -- almost all numbers would be in just a few buckets
. mediocre hash function:
 -- last 3 digits of x
 -- not as bad as area code, but likely to be patterns involved here as well
 -- in other words: not random

. another example:
 using memory locations as keys
 problem is that all memory address will be powers of 2,
 and your number of buckets, 1000, is divisible by 2
 so your hash function using modulus would literally be unable to return odd
 numbers, so half your buckets in your table would ALWAYS be empty

Quick and Dirty Hash Functions
. step 1: convert any non-integers to integers
. step 2: compression function
 -- example, mod n
  -- use modulus based on number of buckets
. your number of buckets n should be unrelated to your data
 -- eg, if you use modulus to compress, and your modulus number
    is divisible by 3 and all your data is divisible by 3, everything
    will go in one bucket
. choose PRIME numbers for n
 -- choose one within constant factor of # of objects
 -- PRIME should not be close to patterns in your data
  -- eg, prime should not be too close to a power of 2 if all of your
     data is a power of 2

Load of a Hash Table
. alpha = # of objects in hash table / # buckets in hash table
 -- more objects in table grows load
 -- more buckets with fixed amount of data drops load
 -- load factors > 1 -- only chaining is option
. constant time lookups require load <= 1
 -- *** ideally well under 1, else performance will degrade
. need a good hash function
 -- unfortunately, perfect hash function does not exist
 -- every hash function has a pathological dataset
 -- why? b/c of nature of compression that hash functions implement

Pathological Data and Hash Functions
. *** Different -- with previous algorithms studied in class, input didn't matter
  -- not so with hash functions
  -- denial of service implications of badly designed hash functions

Solutions for bad hash functions
. crypto hash functions
 -- even cryptographic hash functions -- eg SHA-2 -- have pathological datasets,
    they're just infeasible to reverse engineer
. randomization:
 --  have a FAMILY of available hash functions, but randomly choose one;
     similar to quicksort ON-AVERAGE guarantee; this prevents your app
     from being vulnerable to any one pathology

Universal Hash Functions
. H is set of hash functions
. H is universal iff:
  for all pairs x,y in u, the Pr(x,y collide) <= 1/n
  when h is chosen at random from H
. e.g., probability of collision should be as small as probability that x and y
  assigned to same bucket purely at random

